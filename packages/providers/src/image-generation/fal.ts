import { BaseSyncGenerator } from './base';
import { GenerationRequest, GenerationResponse, ImageGenerationProvider } from '../types';

export interface FALImageConfig extends ImageGenerationProvider {
  providerKey: 'fal';
  model: string;
  apiKey: string;
  baseUrl?: string;
}

export class FALImageGenerator extends BaseSyncGenerator {
  protected config: FALImageConfig;
  private baseUrl: string;

  constructor(config: FALImageConfig) {
    super(config);
    this.config = config;
    this.baseUrl = config.baseUrl || 'https://fal.run';
  }

  async generate(request: GenerationRequest): Promise<GenerationResponse> {
    this.validateRequest(request);

    const input = this.transformInput(request);
    const endpoint = this.getEndpoint();

    const response = await fetch(`${this.baseUrl}/fal-ai/${endpoint}`, {
      method: 'POST',
      headers: {
        Authorization: `Key ${this.config.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(input),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`FAL API error: ${response.status} ${response.statusText} - ${errorText}`);
    }

    const result = await response.json();
    return this.transformOutput(result, request);
  }

  async generateBatch(requests: GenerationRequest[]): Promise<GenerationResponse[]> {
    // FAL supports batch processing, but for simplicity, we'll do parallel individual requests
    return Promise.all(requests.map((request) => this.generate(request)));
  }

  private getEndpoint(): string {
    // Map model to FAL endpoint
    const modelMap: Record<string, string> = {
      'flux/schnell': 'flux/schnell',
      'flux/dev': 'flux/dev',
      'stable-diffusion-xl': 'stable-diffusion-xl-base-1.0',
      'stable-diffusion-3': 'stable-diffusion-v3-medium',
    };

    // If model is a full FAL model ID, use it directly
    if (this.config.model.includes('/')) {
      return this.config.model;
    }

    // Otherwise, try to map it
    return modelMap[this.config.model] || this.config.model;
  }

  private transformInput(request: GenerationRequest): any {
    const input: any = {
      prompt: request.prompt,
      image_size: this.getImageSize(request),
      num_inference_steps: request.steps || 4, // FAL Flux is fast
      guidance_scale: request.guidance || 3.5,
      num_images: request.count || 1,
      enable_safety_checker: true,
    };

    if (request.negativePrompt) {
      input.negative_prompt = request.negativePrompt;
    }

    if (request.seed !== undefined) {
      input.seed = request.seed;
    }

    if (request.inputImage) {
      input.image_url = request.inputImage;
    }

    return input;
  }

  private getImageSize(request: GenerationRequest): string {
    const width = request.width || 1024;
    const height = request.height || 1024;

    // FAL typically uses predefined sizes
    if (request.aspectRatio) {
      const ratioMap: Record<string, string> = {
        '1:1': 'square',
        '16:9': 'landscape_16_9',
        '9:16': 'portrait_9_16',
        '4:3': 'landscape_4_3',
        '3:4': 'portrait_3_4',
      };
      return ratioMap[request.aspectRatio] || 'square';
    }

    if (width === height) {
      return 'square';
    } else if (width > height) {
      return 'landscape_4_3';
    } else {
      return 'portrait_3_4';
    }
  }

  private transformOutput(result: any, request: GenerationRequest): GenerationResponse {
    // FAL returns images in different formats depending on the model
    let images: any[] = [];

    if (result.images && Array.isArray(result.images)) {
      images = result.images;
    } else if (result.image) {
      images = [result.image];
    } else if (result.url) {
      images = [{ url: result.url }];
    }

    if (images.length === 0) {
      throw new Error('No images generated by FAL');
    }

    return {
      outputs: images.map((img: any) => ({
        url: img.url,
        width: img.width || request.width || 1024,
        height: img.height || request.height || 1024,
        format: 'png',
        seed: result.seed,
      })),
      metadata: {
        prompt: request.prompt,
        model: this.config.model,
        provider: 'fal',
        parameters: request,
        usage: {
          processingTime: result.inference_time,
        },
      },
    };
  }
}
