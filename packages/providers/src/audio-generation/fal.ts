import { BaseSyncAudioGenerator } from './base';
import { GenerationRequest, GenerationResponse, AudioGenerationProvider } from '../types';

export interface FALAudioConfig extends AudioGenerationProvider {
  providerKey: 'fal';
  model: string;
  apiKey: string;
  baseUrl?: string;
}

/**
 * FAL音频生成器
 * 实现基于FAL API的音频生成功能
 */
export class FALAudioGenerator extends BaseSyncAudioGenerator {
  protected config: FALAudioConfig;
  private baseUrl: string;

  constructor(config: FALAudioConfig) {
    super(config);
    this.config = config;
    this.baseUrl = config.baseUrl || 'https://fal.run';
  }

  /**
   * 生成音频
   * @param request 生成请求参数
   * @returns 生成响应
   */
  async generate(request: GenerationRequest): Promise<GenerationResponse> {
    this.validateRequest(request);

    const input = this.transformInput(request);
    const endpoint = this.getEndpoint();

    const response = await fetch(`${this.baseUrl}/fal-ai/${endpoint}`, {
      method: 'POST',
      headers: {
        Authorization: `Key ${this.config.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify(input),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(
        `FAL Audio API error: ${response.status} ${response.statusText} - ${errorText}`,
      );
    }

    const result = await response.json();
    return this.transformOutput(result, request);
  }

  /**
   * 批量生成音频
   * @param requests 生成请求参数数组
   * @returns 生成响应数组
   */
  async generateBatch(requests: GenerationRequest[]): Promise<GenerationResponse[]> {
    // FAL支持批量处理，但为了简化，我们使用并行的单个请求
    return Promise.all(requests.map((request) => this.generate(request)));
  }

  /**
   * 获取API端点
   * @returns API端点路径
   */
  private getEndpoint(): string {
    // 将模型映射到FAL端点
    const modelMap: Record<string, string> = {
      'minimax/speech-02-turbo': 'minimax/speech-02-turbo',
      'minimax/speech-02-hd': 'minimax/speech-02-hd',
      'playht/play-dialog': 'playht/play-dialog',
      'google/lyria-2': 'google/lyria-2',
    };

    // 如果模型是完整的FAL模型ID，直接使用
    if (this.config.model.includes('/')) {
      return this.config.model;
    }

    // 否则尝试映射
    return modelMap[this.config.model] || this.config.model;
  }

  /**
   * 转换输入参数
   * @param request 生成请求参数
   * @returns FAL API输入格式
   */
  private transformInput(request: GenerationRequest): any {
    const input: any = {
      prompt: request.prompt,
      duration: request.duration || 10, // 默认10秒
      num_outputs: request.count || 1,
    };

    // 根据模型类型添加特定参数
    if (this.config.model.includes('musicgen')) {
      input.model_version = 'large';
      input.normalization_strategy = 'loudness';
      input.top_k = 250;
      input.top_p = 0.0;
    }

    if (this.config.model.includes('bark')) {
      input.text = request.prompt;
      input.voice_preset = 'v2/en_speaker_6';
    }

    if (request.seed !== undefined) {
      input.seed = request.seed;
    }

    if (request.inputAudio) {
      input.input_audio = request.inputAudio;
    }

    return input;
  }

  /**
   * 转换输出格式
   * @param result FAL API响应
   * @param request 原始请求
   * @returns 标准化的生成响应
   */
  private transformOutput(result: any, request: GenerationRequest): GenerationResponse {
    // FAL根据模型返回不同格式的音频
    let audios: any[] = [];

    if (result.audio_files && Array.isArray(result.audio_files)) {
      audios = result.audio_files;
    } else if (result.audio) {
      audios = [result.audio];
    } else if (result.url) {
      audios = [{ url: result.url }];
    } else if (result.output && Array.isArray(result.output)) {
      audios = result.output;
    }

    if (audios.length === 0) {
      throw new Error('No audio generated by FAL');
    }

    return {
      outputs: audios.map((audio: any) => ({
        url: audio.url || audio,
        duration: request.duration || 10,
        format: 'wav',
        seed: result.seed,
      })),
      metadata: {
        prompt: request.prompt,
        model: this.config.model,
        provider: 'fal',
        parameters: request,
        usage: {
          processingTime: result.inference_time,
        },
      },
    };
  }
}
